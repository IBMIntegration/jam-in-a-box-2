---
# Integration Jam-in-a-Box One-Line Setup
# Usage: oc apply -f https://raw.githubusercontent.com/IBMIntegration/jam-in-a-box-2/main/setup.yaml
#
# This will:
# 1. Create necessary namespaces
# 2. Set up privileged service account
# 3. Clone the repository
# 4. Run main.sh to set up the entire environment
#
# Optional: Create a ConfigMap named 'jam-setup-params' in the default namespace 
# to customize parameters. Example:
#   oc create configmap -n default jam-setup-params --from-literal=parameters="--clean --navigator-password=jam --canary"

# Create jam-in-a-box namespace if it doesn't exist
apiVersion: v1
kind: Namespace
metadata:
  name: jam-in-a-box
---
# ServiceAccount with cluster admin privileges for setup
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jam-setup-sa
  namespace: jam-in-a-box
---
# ClusterRoleBinding to give the service account cluster-admin privileges
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: jam-setup-cluster-admin
subjects:
- kind: ServiceAccount
  name: jam-setup-sa
  namespace: jam-in-a-box
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
---
# ConfigMap containing setup scripts
apiVersion: v1
kind: ConfigMap
metadata:
  name: jam-setup-scripts
  namespace: jam-in-a-box
data:
  logs.sh: |
    #!/bin/bash
    # Logging functions
    
    error() {
      echo "ERROR: $*" >&2
      echo Cannot continue setup.
      exit 1
    }
    
    info() {
      echo "$*"
    }
    
    success() {
      echo "✓ $*"
    }
    
    warning() {
      echo "⚠️  WARNING: $*"
    }

  prereq-tools.sh: |
    #!/bin/bash
    # Install prerequisite tools
    set -e
    
    source /scripts/logs.sh
    
    info "Installing required tools..."
    info "Available repositories:"
    microdnf repolist
    info "Attempting to install packages..."
    microdnf install -y git openssl curl tar gzip findutils procps-ng
    
    # Try to install jq, with fallback if not available in microdnf repos
    info "Installing jq..."
    if ! microdnf install -y jq 2>/dev/null; then
      info "jq not available in microdnf repos, installing from GitHub releases..."
      curl -L "https://github.com/jqlang/jq/releases/latest/download/jq-linux64" \
        -o /usr/local/bin/jq
      chmod +x /usr/local/bin/jq
      success "jq installed from GitHub"
    else
      success "jq installed via microdnf"
    fi
    
    # Verify installations
    info "Verifying tool installations:"
    which git && success "git installed"
    which curl && success "curl installed"
    which jq && success "jq installed"

  oc-tools.sh: |
    #!/bin/bash
    # Install kubectl and oc CLI tools
    set -e
    
    source /scripts/logs.sh
    
    # Install kubectl (compatible with OpenShift)
    info "Installing kubectl..."
    curl -LO "https://dl.k8s.io/release/$(curl -L -s \
      https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    chmod +x kubectl
    mv kubectl /usr/local/bin/
    success "kubectl installed"
    
    # Install OpenShift CLI (oc)
    info "Installing OpenShift CLI (oc)..."
    # Try to get oc from the cluster's console-cli-downloads if available
    if OC_URL=$(kubectl get route console -n openshift-console \
        -o jsonpath='{.spec.host}' 2>/dev/null); then
      info "Attempting to download oc from cluster..."
      curl -kL "https://$OC_URL/api/v1/namespaces/openshift-console/services/downloads/proxy/linux/oc.tar" \
        -o oc.tar 2>/dev/null || info "Cluster download failed, trying GitHub..."
      if [ -f oc.tar ] && tar -tf oc.tar >/dev/null 2>&1; then
        tar -xf oc.tar && mv oc /usr/local/bin/ && rm oc.tar
        success "oc installed from cluster"
      else
        rm -f oc.tar
        info "Downloading oc from GitHub..."
        curl -L "https://github.com/openshift/okd/releases/download/4.14.0-0.okd-2023-12-01-225814/openshift-client-linux-4.14.0-0.okd-2023-12-01-225814.tar.gz" \
          | tar -xzf - oc
        mv oc /usr/local/bin/
        success "oc installed from GitHub"
      fi
    else
      info "Downloading oc from GitHub..."
      curl -L "https://github.com/openshift/okd/releases/download/4.14.0-0.okd-2023-12-01-225814/openshift-client-linux-4.14.0-0.okd-2023-12-01-225814.tar.gz" \
        | tar -xzf - oc
      mv oc /usr/local/bin/
      success "oc installed from GitHub"
    fi

  auth-config.sh: |
    #!/bin/bash
    # Configure OpenShift authentication
    set -e
    
    source /scripts/logs.sh
    
    info "Configuring OpenShift CLI authentication..."
    export KUBECONFIG=/tmp/kubeconfig
    kubectl config set-cluster kubernetes \
      --server=https://kubernetes.default.svc \
      --certificate-authority=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    kubectl config set-credentials serviceaccount \
      --token=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
    kubectl config set-context default \
      --cluster=kubernetes \
      --user=serviceaccount \
      --namespace=jam-in-a-box
    kubectl config use-context default
    
    # Make KUBECONFIG and retry function available
    echo "export KUBECONFIG=/tmp/kubeconfig" >> /tmp/env_setup
    echo "export PATH=/usr/local/bin:\$PATH" >> /tmp/env_setup
    
    # Define retry function in env_setup for use by this and other scripts
    cat >> /tmp/env_setup << 'EOF'
    retry_command() {
      local max_attempts=5
      local delay=10
      local attempt=1
      while [ $attempt -le $max_attempts ]; do
        if "$@" 2>/dev/null; then
          return 0
        elif [[ $? -eq 130 ]]; then
          echo "Command interrupted, stopping retries"
          return 130
        else
          echo "Attempt $attempt/$max_attempts failed, retrying in ${delay}s..."
          sleep $delay
          attempt=$((attempt + 1))
        fi
      done
      echo "All $max_attempts attempts failed"
      return 1
    }
    EOF
    
    # Source env_setup to make retry_command available now
    source /tmp/env_setup
    
    # Verify authentication
    info "Verifying OpenShift authentication..."
    kubectl auth can-i get pods --namespace=jam-in-a-box
    if [ $? -ne 0 ]; then
      error "Failed to authenticate with OpenShift"
    fi
    success "Authentication successful!"
    
    # Test kubectl command specifically
    info "Testing kubectl command..."
    kubectl version --client
    kubectl auth whoami 2>/dev/null || info "ServiceAccount authentication active"

  registry-check.sh: |
    #!/bin/bash
    # Check and wait for OpenShift registry to be ready
    set -e
    
    source /scripts/logs.sh
    source /tmp/env_setup
    
    info "Checking OpenShift image registry state..."
    REGISTRY_STATE=""
    for i in {1..30}; do
      # Use retry logic for registry state check
      REGISTRY_STATE=$(retry_command kubectl get \
        config.imageregistry.operator.openshift.io/cluster \
        -o jsonpath='{.spec.managementState}' || echo "Unknown")
      if [ "$REGISTRY_STATE" = "Managed" ]; then
        success "Registry is in Managed state"
        break
      else
        info "Registry state: $REGISTRY_STATE (attempt $i/30)"
        if [ "$REGISTRY_STATE" = "Removed" ]; then
          info "Attempting to set registry to Managed state..."
          retry_command kubectl patch \
            config.imageregistry.operator.openshift.io/cluster \
            --type merge -p '{"spec":{"managementState":"Managed"}}' \
            || info "Failed to patch registry state"
        fi
        sleep 10
      fi
    done
    
    if [ "$REGISTRY_STATE" != "Managed" ]; then
      warning "Registry state is still $REGISTRY_STATE after 5 minutes"
      warning "This may cause build failures, but continuing setup..."
    fi
    
    # Wait a bit more for the registry to stabilize after state changes
    info "Waiting for registry to stabilize..."
    sleep 30

  repo-clone.sh: |
    #!/bin/bash
    # Clone the jam-in-a-box repository
    set -e
    
    source /scripts/logs.sh
    source /tmp/env_setup
    
    # Check for optional parameters ConfigMap first
    info "Checking for custom parameters..."
    PARAMETERS=""
    if kubectl get configmap jam-setup-params -n default 2>/dev/null; then
      info "Found custom parameters ConfigMap"
      PARAMETERS=$(kubectl get configmap jam-setup-params -n default \
        -o jsonpath='{.data.parameters}' 2>/dev/null || echo "")
    else
      info "No custom parameters found, using defaults"
    fi
    
    info "Parameters to use: ${PARAMETERS:-'(none)'}"
    
    # Determine which branch to clone based on parameters
    BRANCH="main"
    if [[ "$PARAMETERS" == *"--canary"* ]]; then
      BRANCH="canary"
      info "Using canary branch based on --canary parameter"
    else
      info "Using main branch (default)"
    fi
    
    # Get the repository URL from repo-config.json
    repoUrl=""
    curl -L https://raw.githubusercontent.com/IBMIntegration/jam-in-a-box-2/refs/heads/main/repo-config.json \
      -o repo-config.json 2>/dev/null
    repoUrl="$(cat repo-config.json | jq -r '.template_vars.REPO_GIT_URL')"
    if [[ "$PARAMETERS" == *"--fork="* ]]; then
      FORK_NAME=$(echo "$PARAMETERS" | grep -oP '(?<=--fork=)[^ ]+')
      FORK_REPO_URL=$(cat repo-config.json | \
        jq -r --arg FORK_NAME "$FORK_NAME" \
        '.forks[$FORK_NAME].template_vars.REPO_GIT_URL // empty')
      if [ -n "$FORK_REPO_URL" ]; then
        repoUrl="$FORK_REPO_URL"
        info "Using fork repository URL: $repoUrl"
      else
        info "Fork '$FORK_NAME' not found in repo-config.json, using default repository"
      fi
    fi

    # Clone the repository with the determined branch
    info "Cloning repository from branch: $BRANCH"
    git clone -b $BRANCH $repoUrl .
    
    info "Repository cloned successfully. Contents:"
    ls -la
    
    # Make scripts executable
    info "Setting up permissions..."
    find . -name "*.sh" -exec chmod +x {} \;
    chmod +x main.sh
    
    # Export PARAMETERS for main.sh
    echo "export PARAMETERS='${PARAMETERS}'" >> /tmp/env_setup

  setup.sh: |
    #!/bin/bash
    # Main setup orchestration script
    set -e
    
    echo "=== Integration Jam-in-a-Box Setup - Main Phase ==="
    
    # Source logging functions
    source /scripts/logs.sh
    
    # Install prerequisite tools
    /scripts/prereq-tools.sh
    
    # Install kubectl and oc
    /scripts/oc-tools.sh
    
    # Configure authentication
    /scripts/auth-config.sh
    
    # Check registry readiness
    /scripts/registry-check.sh
    
    # Clone repository
    cd /workspace
    /scripts/repo-clone.sh
    
    # Run the main setup script
    info "Starting main setup process..."
    info "Current directory: $(pwd)"
    info "Directory contents:"
    ls -la
    
    if [ -f "./main.sh" ]; then
      # Source the environment setup to ensure KUBECONFIG is available
      source /tmp/env_setup
      
      info "Executing: ./main.sh ${PARAMETERS}"
      
      # Debug: Show current authentication status before running main.sh
      info "Debug: Current authentication status:"
      info "KUBECONFIG=$KUBECONFIG"
      retry_command kubectl auth whoami 2>/dev/null || \
        info "Using ServiceAccount token authentication"
      retry_command kubectl get pods --namespace=jam-in-a-box --limit=1 \
        >/dev/null 2>&1 && success "OpenShift access confirmed" || \
        warning "OpenShift access failed"
      
      # Run main.sh with retry logic for resilience
      if ! retry_command ./main.sh ${PARAMETERS}; then
        error "main.sh failed after multiple retry attempts"
        SETUP_EXIT_CODE=1
      else
        SETUP_EXIT_CODE=0
      fi
      
      if [ $SETUP_EXIT_CODE -eq 0 ]; then
        echo "=== Setup completed successfully! ==="
      else
        echo "=== Setup failed with exit code: $SETUP_EXIT_CODE ==="
      fi
    else
      error "main.sh not found in cloned repository"
      ls -la
      exit 1
    fi
    
    # Keep container running for debugging/testing
    info "Setup phase complete. Container will remain running for inspection."
    info "You can exec into this pod to test changes or debug issues:"
    info "  oc exec -it jam-setup-pod -n jam-in-a-box -- /bin/bash"
    info ""
    info "To clean up when done:"
    info "  oc delete pod jam-setup-pod -n jam-in-a-box"
    info "  oc delete clusterrolebinding jam-setup-cluster-admin"
    info "  oc delete serviceaccount jam-setup-sa -n jam-in-a-box"
    info "  oc delete namespace jam-in-a-box"
    
    # Sleep to keep container alive
    while true; do
      sleep 3600
    done
---
# Main setup pod
apiVersion: v1
kind: Pod
metadata:
  name: jam-setup-pod
  namespace: jam-in-a-box
  labels:
    app: jam-setup
spec:
  serviceAccount: jam-setup-sa
  restartPolicy: Never
  
  # Shared volume for git repository
  volumes:
  - name: repo-volume
    emptyDir: {}
  - name: setup-scripts
    configMap:
      name: jam-setup-scripts
      defaultMode: 0755
  
  # Main container to run setup
  containers:
  - name: jam-setup
    image: registry.redhat.io/ubi8/nodejs-20:latest
    workingDir: /workspace
    volumeMounts:
    - name: repo-volume
      mountPath: /workspace
    - name: setup-scripts
      mountPath: /scripts
    
    # Set up environment
    env:
    - name: HOME
      value: "/tmp"
    
    command: ["/bin/bash", "/scripts/setup.sh"]
    
    # Security context for privileged operations
    securityContext:
      privileged: true
      runAsUser: 0
    
    # Resource limits
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "2Gi"
        cpu: "1000m"
