---
# Integration Jam-in-a-Box One-Line Setup
# Usage: oc apply -f https://raw.githubusercontent.com/IBMIntegration/jam-in-a-box-2/main/setup.yaml
#
# This will:
# 1. Create necessary namespaces
# 2. Set up privileged service account
# 3. Clone the repository
# 4. Run main.sh to set up the entire environment
#
# Optional: Create a ConfigMap named 'jam-setup-params' in the default namespace 
# to customize parameters. Example:
#   oc create configmap -n default jam-setup-params --from-literal=parameters="--clean --navigator-password=jam --canary"

# Create jam-in-a-box namespace if it doesn't exist
apiVersion: v1
kind: Namespace
metadata:
  name: jam-in-a-box
---
# ServiceAccount with cluster admin privileges for setup
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jam-setup-sa
  namespace: jam-in-a-box
---
# ClusterRoleBinding to give the service account cluster-admin privileges
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: jam-setup-cluster-admin
subjects:
- kind: ServiceAccount
  name: jam-setup-sa
  namespace: jam-in-a-box
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
---
# ConfigMap containing setup scripts
apiVersion: v1
kind: ConfigMap
metadata:
  name: jam-setup-scripts
  namespace: jam-in-a-box
data:
  logs.sh: |
    #!/bin/bash
    # Logging functions
    
    error() {
      echo "ERROR: $*" >&2
      echo Cannot continue setup.
      exit 1
    }
    
    info() {
      echo "$*"
    }
    
    success() {
      echo "✓ $*"
    }
    
    warning() {
      echo "⚠️  WARNING: $*"
    }

  prereq-tools.sh: |
    #!/bin/bash
    # Install prerequisite tools
    set -e
    
    source /scripts/logs.sh
    
    info "Installing required tools..."
    info "Available repositories:"
    yum repolist
    info "Attempting to install packages..."
    yum install -y git openssl curl tar gzip findutils procps-ng
    
    # Try to install jq, with fallback if not available in yum repos
    info "Installing jq..."
    if ! yum install -y jq 2>/dev/null; then
      info "jq not available in microdnf repos, installing from GitHub releases..."
      curl -L "https://github.com/jqlang/jq/releases/latest/download/jq-linux64" \
        -o /usr/local/bin/jq
      chmod +x /usr/local/bin/jq
      success "jq installed from GitHub"
    else
      success "jq installed via yum"
    fi
    
    # Verify installations
    info "Verifying tool installations:"
    command -v git >/dev/null && success "git installed"
    command -v curl >/dev/null && success "curl installed"
    command -v jq >/dev/null && success "jq installed"
    command -v node >/dev/null && success "Node.js installed: $(node --version)"
    command -v npm >/dev/null && success "npm installed: $(npm --version)"

  oc-tools.sh: |
    #!/bin/bash
    # Install kubectl and oc CLI tools
    set -e
    
    source /scripts/logs.sh
    
    # Install kubectl (compatible with OpenShift)
    info "Installing kubectl..."
    curl -LO "https://dl.k8s.io/release/$(curl -L -s \
      https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    chmod +x kubectl
    mv kubectl /usr/local/bin/
    success "kubectl installed"
    
    # Install OpenShift CLI (oc)
    info "Installing OpenShift CLI (oc)..."
    # Try to get oc from the cluster's console-cli-downloads if available
    if OC_URL=$(kubectl get route console -n openshift-console \
        -o jsonpath='{.spec.host}' 2>/dev/null); then
      info "Attempting to download oc from cluster..."
      curl -kL "https://$OC_URL/api/v1/namespaces/openshift-console/services/downloads/proxy/linux/oc.tar" \
        -o oc.tar 2>/dev/null || info "Cluster download failed, trying GitHub..."
      if [ -f oc.tar ] && tar -tf oc.tar >/dev/null 2>&1; then
        tar -xf oc.tar && mv oc /usr/local/bin/ && rm oc.tar
        success "oc installed from cluster"
      else
        rm -f oc.tar
        info "Downloading oc from GitHub..."
        curl -L "https://github.com/openshift/okd/releases/download/4.14.0-0.okd-2023-12-01-225814/openshift-client-linux-4.14.0-0.okd-2023-12-01-225814.tar.gz" \
          | tar -xzf - oc
        mv oc /usr/local/bin/
        success "oc installed from GitHub"
      fi
    else
      info "Downloading oc from GitHub..."
      curl -L "https://github.com/openshift/okd/releases/download/4.14.0-0.okd-2023-12-01-225814/openshift-client-linux-4.14.0-0.okd-2023-12-01-225814.tar.gz" \
        | tar -xzf - oc
      mv oc /usr/local/bin/
      success "oc installed from GitHub"
    fi

  auth-config.sh: |
    #!/bin/bash
    # Configure OpenShift authentication
    set -e
    
    source /scripts/logs.sh
    
    info "Configuring OpenShift CLI authentication..."
    export KUBECONFIG=/tmp/kubeconfig
    kubectl config set-cluster kubernetes \
      --server=https://kubernetes.default.svc \
      --certificate-authority=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    kubectl config set-credentials serviceaccount \
      --token=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
    kubectl config set-context default \
      --cluster=kubernetes \
      --user=serviceaccount \
      --namespace=jam-in-a-box
    kubectl config use-context default
    
    # Make KUBECONFIG and retry function available
    echo "export KUBECONFIG=/tmp/kubeconfig" >> /tmp/env_setup
    echo "export PATH=/usr/local/bin:/usr/bin:/bin:\$PATH" >> /tmp/env_setup
    
    # Define retry function in env_setup for use by this and other scripts
    cat >> /tmp/env_setup << 'EOF'
    retry_command() {
      local max_attempts=2
      local delay=10
      local attempt=1
      while [ $attempt -le $max_attempts ]; do
        if "$@"; then
          return 0
        elif [[ $? -eq 130 ]]; then
          echo "Command interrupted, stopping retries"
          return 130
        else
          echo "Attempt $attempt/$max_attempts failed, retrying in ${delay}s..."
          sleep $delay
          attempt=$((attempt + 1))
        fi
      done
      echo "All $max_attempts attempts failed"
      return 1
    }
    EOF
    
    # Source env_setup to make retry_command available now
    source /tmp/env_setup
    
    # Verify authentication
    info "Verifying OpenShift authentication..."
    kubectl auth can-i get pods --namespace=jam-in-a-box
    if [ $? -ne 0 ]; then
      error "Failed to authenticate with OpenShift"
    fi
    success "Authentication successful!"
    
    # Test kubectl command specifically
    info "Testing kubectl command..."
    kubectl version --client
    kubectl auth whoami 2>/dev/null || info "ServiceAccount authentication active"

  registry-check.sh: |
    #!/bin/bash
    # Check and wait for OpenShift registry to be ready
    set -e
    
    source /scripts/logs.sh
    source /tmp/env_setup
    
    info "Checking OpenShift image registry configuration..."
    
    # Check current registry state
    REGISTRY_STATE=$(retry_command kubectl get \
      config.imageregistry.operator.openshift.io/cluster \
      -o jsonpath='{.spec.managementState}' 2>/dev/null || echo "Unknown")
    STORAGE_CONFIG=$(retry_command kubectl get \
      config.imageregistry.operator.openshift.io/cluster \
      -o jsonpath='{.spec.storage}' 2>/dev/null || echo "{}")
    
    info "Current registry state: $REGISTRY_STATE"
    info "Current storage config: $STORAGE_CONFIG"
    
    # If registry is Removed or has no storage, configure it
    if [ "$REGISTRY_STATE" = "Removed" ] || [ "$STORAGE_CONFIG" = "{}" ]; then
      info "Configuring registry with Managed state and PVC storage..."
      
      # Prefer CephFS for RWX support, fall back to default or RBD
      STORAGE_CLASS=""
      if retry_command kubectl get storageclass ocs-external-storagecluster-cephfs >/dev/null 2>&1; then
        STORAGE_CLASS="ocs-external-storagecluster-cephfs"
        info "Found CephFS StorageClass (supports ReadWriteMany)"
      else
        # Get default StorageClass
        STORAGE_CLASS=$(retry_command kubectl get storageclass \
          -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class=="true")].metadata.name}' \
          2>/dev/null || echo "")
        
        if [ -z "$STORAGE_CLASS" ]; then
          warning "No default StorageClass found, using first available..."
          STORAGE_CLASS=$(retry_command kubectl get storageclass \
            -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
        fi
      fi
      
      if [ -n "$STORAGE_CLASS" ]; then
        info "Using StorageClass: $STORAGE_CLASS"
        
        if [ "$STORAGE_CLASS" = "ocs-external-storagecluster-cephfs" ]; then
          # Pre-create PVC with CephFS to override default StorageClass
          info "Pre-creating PVC with CephFS StorageClass..."
          cat <<'EOFPVC' | retry_command kubectl apply -f -
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: image-registry-storage
      namespace: openshift-image-registry
      annotations:
        imageregistry.openshift.io: "true"
    spec:
      accessModes:
      - ReadWriteMany
      resources:
        requests:
          storage: 100Gi
      storageClassName: ocs-external-storagecluster-cephfs
    EOFPVC
          if [ $? -eq 0 ]; then
            success "PVC pre-created with CephFS"
            # Now configure registry to use the pre-created PVC
            retry_command kubectl patch \
              config.imageregistry.operator.openshift.io/cluster \
              --type merge -p '{"spec":{"managementState":"Managed","storage":{"pvc":{"claim":"image-registry-storage"}}}}' \
              || error "Failed to configure registry"
            success "Registry configured with Managed state and CephFS storage (RWX)"
          else
            error "Failed to pre-create PVC"
          fi
        else
          # Use default StorageClass with RWO mode
          retry_command kubectl patch \
            config.imageregistry.operator.openshift.io/cluster \
            --type merge -p "{\"spec\":{\"managementState\":\"Managed\",\"replicas\":1,\"rolloutStrategy\":\"Recreate\",\"storage\":{\"pvc\":{\"claim\":\"\"}}}}" \
            || error "Failed to configure registry"
          success "Registry configured with Managed state and PVC storage (RWO)"
        fi
      else
        error "No StorageClass available for registry PVC"
      fi
    else
      success "Registry already configured (state: $REGISTRY_STATE)"
    fi
    
    # Wait for PVC to be created and bound
    info "Waiting for registry PVC to be created..."
    for i in {1..36}; do
      PVC_NAME=$(retry_command kubectl get pvc -n openshift-image-registry \
        -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
      if [ -n "$PVC_NAME" ]; then
        success "PVC created: $PVC_NAME"
        break
      fi
      info "Waiting for PVC creation (attempt $i/36)..."
      sleep 5
    done
    
    if [ -z "$PVC_NAME" ]; then
      warning "PVC not created after 3 minutes, checking registry config..."
      retry_command kubectl get config.imageregistry.operator.openshift.io/cluster -o yaml
    else
      # Wait for PVC to be bound
      info "Waiting for PVC to be bound..."
      for i in {1..60}; do
        PVC_STATUS=$(retry_command kubectl get pvc "$PVC_NAME" \
          -n openshift-image-registry \
          -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
        if [ "$PVC_STATUS" = "Bound" ]; then
          success "PVC is bound"
          break
        fi
        info "PVC status: $PVC_STATUS (attempt $i/60)..."
        sleep 5
      done
      
      if [ "$PVC_STATUS" != "Bound" ]; then
        warning "PVC not bound after 5 minutes"
        retry_command kubectl describe pvc "$PVC_NAME" -n openshift-image-registry
      fi
    fi
    
    # Wait for registry deployment to be created
    info "Waiting for registry deployment..."
    for i in {1..24}; do
      DEPLOYMENT_EXISTS=$(retry_command kubectl get deployment \
        image-registry -n openshift-image-registry \
        -o jsonpath='{.metadata.name}' 2>/dev/null || echo "")
      if [ -n "$DEPLOYMENT_EXISTS" ]; then
        success "Registry deployment created"
        break
      fi
      info "Waiting for deployment creation (attempt $i/24)..."
      sleep 5
    done
    
    if [ -z "$DEPLOYMENT_EXISTS" ]; then
      warning "Registry deployment not created after 2 minutes"
    else
      # Wait for registry pods to be ready
      info "Waiting for registry pods to be ready..."
      for i in {1..60}; do
        READY_REPLICAS=$(retry_command kubectl get deployment \
          image-registry -n openshift-image-registry \
          -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
        DESIRED_REPLICAS=$(retry_command kubectl get deployment \
          image-registry -n openshift-image-registry \
          -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "1")
        
        if [ "$READY_REPLICAS" = "$DESIRED_REPLICAS" ] && [ "$READY_REPLICAS" != "0" ]; then
          success "Registry is ready with $READY_REPLICAS/$DESIRED_REPLICAS replicas"
          break
        else
          info "Registry replicas: $READY_REPLICAS/$DESIRED_REPLICAS ready (attempt $i/60)..."
          sleep 5
        fi
      done
      
      if [ "$READY_REPLICAS" != "$DESIRED_REPLICAS" ]; then
        warning "Registry may not be fully ready, checking pod status..."
        retry_command kubectl get pods -n openshift-image-registry
        retry_command kubectl get events -n openshift-image-registry --sort-by=.metadata.creationTimestamp | tail -20
      fi
    fi
    
    # Final verification
    info "Verifying registry operator status..."
    OPERATOR_AVAILABLE=$(retry_command kubectl get \
      config.imageregistry.operator.openshift.io/cluster \
      -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' 2>/dev/null || echo "Unknown")
    
    if [ "$OPERATOR_AVAILABLE" = "True" ]; then
      success "Registry operator reports Available=True"
    else
      warning "Registry operator status: Available=$OPERATOR_AVAILABLE"
    fi

  repo-clone.sh: |
    #!/bin/bash
    # Clone the jam-in-a-box repository
    set -e
    
    source /scripts/logs.sh
    source /tmp/env_setup
    
    # Check if archive-helper pod exists in jam-in-a-box namespace
    if kubectl get service archive-helper -n jam-in-a-box >/dev/null 2>&1; then

      warning "Found archive-helper service, using archive extraction method"
    
      ARCHIVE_URL=http://archive-helper.jam-in-a-box.svc.cluster.local/jam-in-a-box.tar
      info "Downloading repository archive from: $ARCHIVE_URL"
      
      # Download and extract the tarball
      curl -L "$ARCHIVE_URL" | tar -xf - --strip-components=1
      
      success "Repository archive downloaded and extracted"
      info "Repository contents:"
      ls -la
      
    else

      # Determine which branch to clone based on parameters
      BRANCH="main"
      if [[ "$PARAMETERS" == *"--canary="* ]]; then
        BRANCH=$(echo "$PARAMETERS" | grep -oP '(?<=--canary=)[^ ]+')
        info "Using branch: $BRANCH (from --canary parameter)"
      elif [[ "$PARAMETERS" == *"--canary"* ]]; then
        BRANCH="canary"
        info "Using canary branch based on --canary parameter"
      else
        info "Using main branch (default)"
      fi
      
      # Get the repository URL from repo-config.json
      repoUrl=""
      curl -L https://raw.githubusercontent.com/IBMIntegration/jam-in-a-box-2/refs/heads/main/repo-config.json \
        -o repo-config.json 2>/dev/null
      repoUrl="$(cat repo-config.json | jq -r '.template_vars.REPO_GIT_URL')"
      if [[ "$PARAMETERS" == *"--fork="* ]]; then
        FORK_NAME=$(echo "$PARAMETERS" | grep -oP '(?<=--fork=)[^ ]+')
        FORK_REPO_URL=$(cat repo-config.json | \
          jq -r --arg FORK_NAME "$FORK_NAME" \
          '.forks[$FORK_NAME].template_vars.REPO_GIT_URL // empty')
        if [ -n "$FORK_REPO_URL" ]; then
          repoUrl="$FORK_REPO_URL"
          info "Using fork repository URL: $repoUrl"
        else
          info "Fork '$FORK_NAME' not found in repo-config.json, using default repository"
        fi
      fi

      # Clone the repository with the determined branch
      info "Cloning repository from branch: $BRANCH"
      rm -rf .git ./* .[^.]* 2>/dev/null || true
      git clone -b $BRANCH $repoUrl .
      
      info "Repository cloned successfully. Contents:"
      ls -la
    fi
    
    # Make scripts executable
    info "Setting up permissions..."
    find . -name "*.sh" -exec chmod +x {} \;
    chmod +x main.sh
    
  setup.sh: |
    #!/bin/bash
    # Main setup orchestration script
    set -e
    
    # Source logging functions
    source /scripts/logs.sh
    
    # Install prerequisite tools
    /scripts/prereq-tools.sh
    
    # Install kubectl and oc
    /scripts/oc-tools.sh
    
    # Configure authentication
    /scripts/auth-config.sh

    # Check for optional parameters ConfigMap first
    info "Checking for custom parameters..."
    PARAMETERS=""
    if kubectl get configmap jam-setup-params -n default; then
      info "Found custom parameters ConfigMap"
      PARAMETERS=$(kubectl get configmap jam-setup-params -n default \
        -o jsonpath='{.data.parameters}' 2>/dev/null || echo "")
    else
      info "No custom parameters found, using defaults"
    fi
    
    info "Parameters to use: ${PARAMETERS:-'(none)'}"
    echo "export PARAMETERS='${PARAMETERS}'" >> /tmp/env_setup

    # Set DEBUG flag based on parameters
    if [[ "$PARAMETERS" == *"--debug"* ]]; then
      DEBUG=true
    else
      DEBUG=${DEBUG:-false}
    fi
    echo "export DEBUG=$DEBUG" >> /tmp/env_setup

    echo "=== Integration Jam-in-a-Box Setup - Main Phase ==="
    
    # Check registry readiness
    /scripts/registry-check.sh
    
    # Clone repository
    cd /workspace
    /scripts/repo-clone.sh
    
    # Run the main setup script
    info "Starting main setup process..."
    info "Current directory: $(pwd)"
    info "Directory contents:"
    ls -la
    
    if [ -f "./main.sh" ]; then
      # Source the environment setup to ensure KUBECONFIG is available
      source /tmp/env_setup
      
      info "Executing: ./main.sh ${PARAMETERS}"
      info "/tmp/env_setup: $(cat /tmp/env_setup)"
      
      # Debug: Show current authentication status before running main.sh
      info "Debug: Current authentication status:"
      info "KUBECONFIG=$KUBECONFIG"
      retry_command kubectl auth whoami 2>/dev/null || \
        info "Using ServiceAccount token authentication"
      retry_command kubectl get pods --namespace=jam-in-a-box --limit=1 \
        >/dev/null 2>&1 && success "OpenShift access confirmed" || \
        warning "OpenShift access failed"
      
      # Run main.sh with retry logic for resilience
      # Ensure PATH is exported for subshells
      export PATH="/usr/local/bin:$PATH"
      if ! retry_command bash -c "source /tmp/env_setup && ./main.sh ${PARAMETERS}"; then
        error "main.sh failed after multiple retry attempts"
        SETUP_EXIT_CODE=1
      else
        SETUP_EXIT_CODE=0
      fi
      
      if [ $SETUP_EXIT_CODE -eq 0 ]; then
        echo "=== Setup completed successfully! ==="
        
        # Wait for navigator pod to be ready
        info "Waiting for jam-in-a-box navigator pod to be ready..."
        for i in {1..60}; do
          NAV_READY=$(retry_command kubectl get pods -n jam-in-a-box \
            -l app=navigator \
            -o jsonpath='{.items[0].status.conditions[?(@.type=="Ready")].status}' \
            2>/dev/null || echo "")
          
          if [ "$NAV_READY" = "True" ]; then
            success "Navigator pod is ready!"
            break
          else
            info "Waiting for navigator pod (attempt $i/60)..."
            sleep 5
          fi
        done
        
        if [ "$NAV_READY" != "True" ]; then
          warning "Navigator pod not ready after 5 minutes"
          retry_command kubectl get pods -n jam-in-a-box -l app=navigator
        else
          echo "=== jam-in-a-box is fully deployed and ready! ==="
        fi
      else
        echo "=== Setup failed with exit code: $SETUP_EXIT_CODE ==="
      fi
    else
      error "main.sh not found in cloned repository"
      ls -la
      exit 1
    fi
    
    # Keep container running for debugging/testing
    if $DEBUG; then
      info "Setup phase complete. Container will remain running for inspection."
      info "You can exec into this pod to test changes or debug issues:"
      info "  oc exec -it jam-setup-pod -n jam-in-a-box -- /bin/bash"
    else
      info "Setup phase complete. Container will exit shortly."
    fi
    info ""
    info "To clean up when done:"
    info "  oc delete deployment jam-in-a-box -n jam-in-a-box"
    info "  oc delete pod jam-setup-pod -n jam-in-a-box"
    info "  oc delete clusterrolebinding jam-setup-cluster-admin"
    info "  oc delete serviceaccount jam-setup-sa -n jam-in-a-box"
    info "  oc delete secrets --field-selector type=Opaque -o name -n jam-in-a-box"
    info "  oc delete -n jam-in-a-box cm jam-setup-scripts \\"
    info "    navigator-md-handler-config navigator-nginx-config \\"
    info "    navigator-scripts-init setup-output"
    
    echo
    echo "=== Access the navigator at: ==="
    echo $'\x1b[4;1m'
    navigator_hostname="$(
      kubectl -n jam-in-a-box get cm setup-output \
        -o jsonpath='{.data.setup\.json}' |
      jq -r '.[]
        |select(.kind=="Route" and .metadata.labels.app=="navigator")
        |.spec.host')"
    navigator_credentials=$(
      kubectl -n jam-in-a-box get secrets setup-secrets \
        -o jsonpath='{.data.secret\.json}' | base64 -d |
      jq -r '.[]|select(.metadata.name=="navigator-credentials")|.data')
    nuser=$(echo "$navigator_credentials" | jq -r '.username')
    npass=$(echo "$navigator_credentials" | jq -r '.password')
    echo "  https://$nuser:$npass@$navigator_hostname"
    echo $'\x1b[0m'
    echo "If credentials are required, use the following:"
    echo "  Username: $nuser"
    echo "  Password: $npass"
    echo
    if $DEBUG; then
      info "DEBUG mode is enabled, keeping container running indefinitely."
      while true; do
        sleep 3600
      done
    else
      info "DEBUG mode is disabled, exiting container."
      exit $SETUP_EXIT_CODE
    fi
---
# Main setup pod
apiVersion: v1
kind: Pod
metadata:
  name: jam-setup-pod
  namespace: jam-in-a-box
  labels:
    app: jam-setup
spec:
  serviceAccount: jam-setup-sa
  restartPolicy: Never
  
  # Shared volume for git repository
  volumes:
  - name: repo-volume
    emptyDir: {}
  - name: setup-scripts
    configMap:
      name: jam-setup-scripts
      defaultMode: 0755
  
  # Main container to run setup
  containers:
  - name: jam-setup
    image: registry.redhat.io/ubi8/nodejs-20:latest
    workingDir: /workspace
    volumeMounts:
    - name: repo-volume
      mountPath: /workspace
    - name: setup-scripts
      mountPath: /scripts
    
    # Set up environment
    env:
    - name: HOME
      value: "/tmp"
    
    command: ["/bin/bash", "/scripts/setup.sh"]
    
    # Security context for privileged operations
    securityContext:
      privileged: true
      runAsUser: 0
    
    # Resource limits
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "2Gi"
        cpu: "1000m"
