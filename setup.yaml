---
# Integration Jam-in-a-Box One-Line Setup
# Usage: oc apply -f https://raw.githubusercontent.com/IBMIntegration/jam-in-a-box-2/main/setup.yaml
#
# This will:
# 1. Create necessary namespaces
# 2. Set up privileged service account
# 3. Clone the repository
# 4. Run main.sh to set up the entire environment
#
# Optional: Create a ConfigMap named 'jam-setup-params' in the default namespace 
# to customize parameters. Example:
#   oc create configmap -n default jam-setup-params --from-literal=parameters="--navigator-password=jam --canary"

# Create jam-in-a-box namespace if it doesn't exist
apiVersion: v1
kind: Namespace
metadata:
  name: jam-in-a-box
---
# ServiceAccount with cluster admin privileges for setup
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jam-setup-sa
  namespace: jam-in-a-box
---
# ClusterRoleBinding to give the service account cluster-admin privileges
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: jam-setup-cluster-admin
subjects:
- kind: ServiceAccount
  name: jam-setup-sa
  namespace: jam-in-a-box
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
---
# ConfigMap containing setup scripts
apiVersion: v1
kind: ConfigMap
metadata:
  name: jam-setup-scripts
  namespace: jam-in-a-box
data:
  logs.sh: |
    #!/bin/bash
    # Logging functions
    
    error() {
      echo "ERROR: $*" >&2
      echo Cannot continue setup.
      exit 1
    }
    
    info() {
      echo "$*"
    }
    
    success() {
      echo "✓ $*"
    }
    
    warning() {
      echo "⚠️  WARNING: $*"
    }

  prereq-tools.sh: |
    #!/bin/bash
    # Install prerequisite tools
    set -e
    
    source /scripts/logs.sh
    
    info "Installing required tools..."
    info "Available repositories:"
    yum repolist
    info "Attempting to install packages..."
    yum install -y git openssl curl tar gzip findutils procps-ng
    
    # Try to install jq, with fallback if not available in yum repos
    info "Installing jq..."
    if ! yum install -y jq 2>/dev/null; then
      info "jq not available in microdnf repos, installing from GitHub releases..."
      curl -L "https://github.com/jqlang/jq/releases/latest/download/jq-linux64" \
        -o /usr/local/bin/jq
      chmod +x /usr/local/bin/jq
      success "jq installed from GitHub"
    else
      success "jq installed via yum"
    fi
    
    # Verify installations
    info "Verifying tool installations:"
    command -v git >/dev/null && success "git installed"
    command -v curl >/dev/null && success "curl installed"
    command -v jq >/dev/null && success "jq installed"
    command -v node >/dev/null && success "Node.js installed: $(node --version)"
    command -v npm >/dev/null && success "npm installed: $(npm --version)"

  oc-tools.sh: |
    #!/bin/bash
    # Install kubectl, oc, and apic CLI tools
    set -e
    
    source /scripts/logs.sh
    
    # Install kubectl (compatible with OpenShift)
    info "Installing kubectl..."
    curl -LO "https://dl.k8s.io/release/$(curl -L -s \
      https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
    chmod +x kubectl
    mv kubectl /usr/local/bin/
    success "kubectl installed"
    
    # Install OpenShift CLI (oc)
    info "Installing OpenShift CLI (oc)..."
    # Try to get oc from the cluster's console-cli-downloads if available
    if OC_URL=$(kubectl get route console -n openshift-console \
        -o jsonpath='{.spec.host}' 2>/dev/null); then
      info "Attempting to download oc from cluster..."
      curl -kL "https://$OC_URL/api/v1/namespaces/openshift-console/services/downloads/proxy/linux/oc.tar" \
        -o oc.tar 2>/dev/null || info "Cluster download failed, trying GitHub..."
      if [ -f oc.tar ] && tar -tf oc.tar >/dev/null 2>&1; then
        tar -xf oc.tar && mv oc /usr/local/bin/ && rm oc.tar
        success "oc installed from cluster"
      else
        rm -f oc.tar
        info "Downloading oc from GitHub..."
        curl -L "https://github.com/openshift/okd/releases/download/4.14.0-0.okd-2023-12-01-225814/openshift-client-linux-4.14.0-0.okd-2023-12-01-225814.tar.gz" \
          | tar -xzf - oc
        mv oc /usr/local/bin/
        success "oc installed from GitHub"
      fi
    else
      info "Downloading oc from GitHub..."
      curl -L "https://github.com/openshift/okd/releases/download/4.14.0-0.okd-2023-12-01-225814/openshift-client-linux-4.14.0-0.okd-2023-12-01-225814.tar.gz" \
        | tar -xzf - oc
      mv oc /usr/local/bin/
      success "oc installed from GitHub"
    fi

    # Only download from cluster as API Connect cluster should already be
    # confirmed installed
    APIM_URL=$(kubectl get route apim-demo-mgmt-api-manager -n tools \
        -o jsonpath='{.spec.host}' 2>/dev/null)
    info "Downloading apic from cluster..."
    curl -kL "https://$APIM_URL/client-downloads/toolkit-linux.tgz" \
      -o toolkit-linux.tgz 2>/dev/null || info "apic download failed"
    if [ -f toolkit-linux.tgz ] && tar -tf toolkit-linux.tgz >/dev/null 2>&1; then
      tar -xf toolkit-linux.tgz && mv apic /usr/local/bin/ && rm toolkit-linux.tgz
      success "apic installed from cluster"
      yes | apic version && success "apic CLI is functional"
    else
      rm -f toolkit-linux.tgz
      warning "apic CLI not installed - API Connect may not be available"
    fi    

  validate-pipeline-run.sh: |
    #!/bin/bash
    # Validate that the cluster provisioning pipeline completed successfully
    set -e
    
    source /scripts/logs.sh

    info "Validating cluster provisioning pipeline..."
    
    # Check if cp4i-demo pipeline exists
    if ! kubectl get pipeline cp4i-demo -n default >/dev/null 2>&1; then
      echo "===============================================" >&2
      echo "UNSUPPORTED ENVIRONMENT" >&2
      echo "===============================================" >&2
      echo "" >&2
      echo "This cluster does not have the cp4i-demo pipeline." >&2
      echo "" >&2
      echo "Jam-in-a-Box only supports IBM Technology Zone" >&2
      echo "'CP4I on OCP-V (2.0)' environments." >&2
      echo "" >&2
      echo "Please request the correct environment type from:" >&2
      echo "https://techzone.ibm.com/" >&2
      echo "" >&2
      echo "===============================================" >&2
      error "Unsupported cluster environment"
    fi
    
    # Find the most recent PipelineRun for cp4i-demo
    PIPELINE_RUN=$(kubectl get pipelinerun -n default \
      -l tekton.dev/pipeline=cp4i-demo \
      --sort-by=.metadata.creationTimestamp \
      -o jsonpath='{.items[-1:].metadata.name}' 2>/dev/null || echo "")
    
    if [ -z "$PIPELINE_RUN" ]; then
      echo "===============================================" >&2
      echo "NO PIPELINE RUNS FOUND" >&2
      echo "===============================================" >&2
      echo "" >&2
      echo "The cp4i-demo pipeline exists but has never been run." >&2
      echo "" >&2
      echo "This indicates the cluster provisioning was not started." >&2
      echo "Please contact IBM Technology Zone support." >&2
      echo "" >&2
      echo "===============================================" >&2
      error "No pipeline runs found"
    fi
    
    info "Found PipelineRun: $PIPELINE_RUN"
    
    # Get PipelineRun status
    PIPELINE_STATUS=$(kubectl get pipelinerun "$PIPELINE_RUN" -n default \
      -o jsonpath='{.status.conditions[?(@.type=="Succeeded")].status}' 2>/dev/null || echo "Unknown")
    PIPELINE_REASON=$(kubectl get pipelinerun "$PIPELINE_RUN" -n default \
      -o jsonpath='{.status.conditions[?(@.type=="Succeeded")].reason}' 2>/dev/null || echo "Unknown")
    
    # Count completed tasks
    TOTAL_TASKS=$(kubectl get pipelinerun "$PIPELINE_RUN" -n default \
      -o json 2>/dev/null | jq -r '[.status.childReferences[]? | select(.kind=="TaskRun")] | length' || echo "0")
    SUCCESSFUL_TASKS=$(kubectl get pipelinerun "$PIPELINE_RUN" -n default \
      -o json 2>/dev/null | jq -r '[.status.childReferences[]? | select(.kind=="TaskRun")] | map(select(.status.conditions[]? | select(.type=="Succeeded" and .status=="True"))) | length' || echo "0")
    
    info "Pipeline status: $PIPELINE_STATUS (Reason: $PIPELINE_REASON)"
    info "Tasks completed: $SUCCESSFUL_TASKS / $TOTAL_TASKS"
    
    # Expected minimum number of tasks for a successful cp4i-demo run
    EXPECTED_MIN_TASKS=20
    
    # Check for failure conditions
    if [ "$PIPELINE_STATUS" = "False" ]; then
      echo "===============================================" >&2
      echo "CLUSTER PROVISIONING PIPELINE FAILED" >&2
      echo "===============================================" >&2
      echo "" >&2
      echo "The cp4i-demo pipeline run ($PIPELINE_RUN) failed during cluster provisioning." >&2
      echo "Status: $PIPELINE_STATUS" >&2
      echo "Reason: $PIPELINE_REASON" >&2
      echo "Tasks completed: $SUCCESSFUL_TASKS / $TOTAL_TASKS" >&2
      echo "" >&2
      echo "This indicates the IBM Cloud Pak for Integration was not properly installed." >&2
      echo "" >&2
      echo "REMEDIATION OPTIONS:" >&2
      echo "1. Check the PipelineRun logs:" >&2
      echo "   oc logs -n default \$(oc get pods -n default -l tekton.dev/pipelineRun=$PIPELINE_RUN \\" >&2
      echo "     --sort-by=.metadata.creationTimestamp -o name | tail -1) --all-containers" >&2
      echo "" >&2
      echo "2. View the PipelineRun details:" >&2
      echo "   oc describe pipelinerun $PIPELINE_RUN -n default" >&2
      echo "" >&2
      echo "3. If the pipeline cannot be fixed, you may need to:" >&2
      echo "   - Request a new cluster from IBM Technology Zone" >&2
      echo "   - Or manually complete the failed installation steps" >&2
      echo "" >&2
      echo "===============================================" >&2
      error "Cluster provisioning pipeline failed - cannot continue"
    fi
    
    # Check if pipeline is still running or incomplete
    if [ "$TOTAL_TASKS" -lt "$EXPECTED_MIN_TASKS" ]; then
      echo "===============================================" >&2
      echo "CLUSTER PROVISIONING IN PROGRESS" >&2
      echo "===============================================" >&2
      echo "" >&2
      echo "The cp4i-demo pipeline run ($PIPELINE_RUN) is still running" >&2
      echo "or has not completed all tasks yet." >&2
      echo "" >&2
      echo "Current status: $PIPELINE_STATUS (Reason: $PIPELINE_REASON)" >&2
      echo "Tasks completed: $TOTAL_TASKS / expected $EXPECTED_MIN_TASKS+" >&2
      echo "" >&2
      echo "The IBM Cloud Pak for Integration is still being installed." >&2
      echo "" >&2
      echo "PLEASE WAIT:" >&2
      echo "1. Monitor the pipeline progress:" >&2
      echo "   oc get pipelinerun $PIPELINE_RUN -n default -w" >&2
      echo "" >&2
      echo "2. Or watch the pods:" >&2
      echo "   oc get pods -n default -l tekton.dev/pipelineRun=$PIPELINE_RUN -w" >&2
      echo "" >&2
      echo "3. The pipeline typically takes 2-3 hours to complete." >&2
      echo "   Re-run jam-in-a-box setup after the pipeline finishes successfully." >&2
      echo "" >&2
      echo "===============================================" >&2
      error "Cluster provisioning still in progress - please wait"
    fi
    
    success "Pipeline validation passed: $TOTAL_TASKS tasks completed successfully"
    exit 0

  auth-config.sh: |
    #!/bin/bash
    # Configure OpenShift authentication
    set -e
    
    source /scripts/logs.sh
    
    info "Configuring OpenShift CLI authentication..."
    export KUBECONFIG=/tmp/kubeconfig
    kubectl config set-cluster kubernetes \
      --server=https://kubernetes.default.svc \
      --certificate-authority=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    kubectl config set-credentials serviceaccount \
      --token=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
    kubectl config set-context default \
      --cluster=kubernetes \
      --user=serviceaccount \
      --namespace=jam-in-a-box
    kubectl config use-context default
    
    # Make KUBECONFIG and retry function available
    echo "export KUBECONFIG=/tmp/kubeconfig" >> /tmp/env_setup
    echo "export PATH=/usr/local/bin:/usr/bin:/bin:\$PATH" >> /tmp/env_setup
    
    # Define retry function in env_setup for use by this and other scripts
    cat >> /tmp/env_setup << 'EOF'
    retry_command() {
      local max_attempts=2
      local delay=10
      local attempt=1
      while [ $attempt -le $max_attempts ]; do
        if "$@"; then
          return 0
        elif [[ $? -eq 130 ]]; then
          echo "Command interrupted, stopping retries"
          return 130
        else
          echo "Attempt $attempt/$max_attempts failed, retrying in ${delay}s..."
          sleep $delay
          attempt=$((attempt + 1))
        fi
      done
      echo "All $max_attempts attempts failed"
      return 1
    }
    EOF
    
    # Source env_setup to make retry_command available now
    source /tmp/env_setup
    
    # Verify authentication
    info "Verifying OpenShift authentication..."
    kubectl auth can-i get pods --namespace=jam-in-a-box
    if [ $? -ne 0 ]; then
      error "Failed to authenticate with OpenShift"
    fi
    success "Authentication successful!"
    
    # Test kubectl command specifically
    info "Testing kubectl command..."
    kubectl version --client
    kubectl auth whoami 2>/dev/null || info "ServiceAccount authentication active"

  registry-check.sh: |
    #!/bin/bash
    # ========================================================================
    # OpenShift Image Registry Configuration Script
    # ========================================================================
    # 
    # PURPOSE:
    #   Configure the OpenShift internal image registry with persistent
    #   storage for use with BuildConfigs and ImageStreams.
    #
    # REQUIREMENTS:
    #   1. Registry must be in Managed state
    #   2. Registry must have PVC-based persistent storage configured
    #   3. Storage must NOT have nested managementState field (causes build
    #      failures with "registry not configured" errors)
    #   4. PVC must use appropriate StorageClass and access mode:
    #      - CephFS: ReadWriteMany supported
    #      - RBD: ReadWriteOnce only (requires single replica + Recreate)
    #   5. Build controller must be able to resolve ImageStreams
    #   6. Controller-manager must have current registry connection
    #
    # APPROACH:
    #   Phase 1: Determine storage requirements based on available classes
    #   Phase 2: Assess current storage state and identify issues
    #   Phase 3-4: Execute storage fixes (always recreate for clean state)
    #   Phase 5-6: Configure registry to Managed state and verify ready
    #   Phase 7: Verify build functionality with test ImageStream
    #
    # ========================================================================
    
    set -e
    
    source /scripts/logs.sh
    source /tmp/env_setup
    
    info "=== Configuring OpenShift Image Registry ==="
    
    # ========================================================================
    # PHASE 1: DETERMINE STORAGE REQUIREMENTS
    # ========================================================================
    # Detect available StorageClasses and determine appropriate access mode,
    # replica count, and rollout strategy based on StorageClass capabilities.
    # ========================================================================
    
    info "Phase 1: Determining storage requirements..."
    
    # Check if CephFS is available (supports RWX)
    if retry_command kubectl get storageclass \
        ocs-external-storagecluster-cephfs >/dev/null 2>&1; then
      STORAGE_CLASS="ocs-external-storagecluster-cephfs"
      ACCESS_MODE="ReadWriteMany"
      REPLICAS=1
      ROLLOUT_STRATEGY="RollingUpdate"
      info "  StorageClass: $STORAGE_CLASS"
      info "  Access Mode: $ACCESS_MODE (CephFS supports RWX)"
      info "  Replicas: $REPLICAS"
      info "  Rollout Strategy: $ROLLOUT_STRATEGY"
    else
      # Use default or first available StorageClass (likely RBD)
      STORAGE_CLASS=$(retry_command kubectl get storageclass \
        2>/dev/null | grep default | awk '{print $1}' | head -n1)
      
      if [ -z "$STORAGE_CLASS" ]; then
        warning "No default StorageClass, using first available..."
        STORAGE_CLASS=$(retry_command kubectl get storageclass -o name \
          2>/dev/null | head -n1 | cut -d/ -f2)
      fi
      
      if [ -z "$STORAGE_CLASS" ]; then
        error "No StorageClass available for registry"
        exit 1
      fi
      
      # RBD only supports RWO for filesystem mode
      ACCESS_MODE="ReadWriteOnce"
      REPLICAS=1
      ROLLOUT_STRATEGY="Recreate"
      info "  StorageClass: $STORAGE_CLASS"
      info "  Access Mode: $ACCESS_MODE (RWO required for RBD)"
      info "  Replicas: $REPLICAS (required for RWO)"
      info "  Rollout Strategy: $ROLLOUT_STRATEGY (required for RWO)"
    fi
    
    # ========================================================================
    # PHASE 2: ASSESS CURRENT STORAGE STATE
    # ========================================================================
    # Check registry spec and status for storage issues including:
    # - Missing storage configuration
    # - Nested managementState bug
    # - Existing PVC state
    # ========================================================================
    
    info "Phase 2: Assessing current storage state..."
    
    REGISTRY_SPEC=$(retry_command kubectl get \
      config.imageregistry.operator.openshift.io/cluster \
      -o json 2>/dev/null)
    
    CURRENT_MGMT_STATE=$(echo "$REGISTRY_SPEC" | \
      jq -r '.spec.managementState // "Unknown"')
    CURRENT_STORAGE=$(echo "$REGISTRY_SPEC" | \
      jq -r '.spec.storage // {}')
    
    info "  Current managementState: $CURRENT_MGMT_STATE"
    info "  Current storage config: $CURRENT_STORAGE"
    
    # Check for the critical nested managementState bug
    NESTED_MGMT=$(echo "$CURRENT_STORAGE" | \
      jq -r '.managementState // "none"')
    
    STORAGE_NEEDS_FIX=false
    
    if [ "$NESTED_MGMT" != "none" ]; then
      warning "  ⚠ Found nested managementState: $NESTED_MGMT"
      warning "  This causes builds to fail - will fix"
      STORAGE_NEEDS_FIX=true
    fi
    
    # Check PVC state
    if kubectl get pvc image-registry-storage \
        -n openshift-image-registry >/dev/null 2>&1; then
      PVC_PHASE=$(kubectl get pvc image-registry-storage \
        -n openshift-image-registry \
        -o jsonpath='{.status.phase}' 2>/dev/null)
      info "  Existing PVC found: phase=$PVC_PHASE"
      info "  Will recreate PVC for clean state"
      STORAGE_NEEDS_FIX=true
    else
      info "  No existing PVC found"
      STORAGE_NEEDS_FIX=true
    fi
    
    # ========================================================================
    # PHASE 3-4: EXECUTE STORAGE FIXES
    # ========================================================================
    # Always recreate storage from scratch to ensure clean state:
    # 1. Clear any existing storage config
    # 2. Delete existing PVC if present  
    # 3. Create fresh PVC with correct specs
    # 4. Configure registry to use the PVC
    # ========================================================================
    
    if [ "$STORAGE_NEEDS_FIX" = "true" ]; then
      info "Phase 3-4: Fixing storage configuration..."
      
      # Step 1: Set registry to Removed to release the PVC
      info "  Step 1: Setting registry to Removed state to release storage..."
      retry_command kubectl patch \
        config.imageregistry.operator.openshift.io/cluster \
        --type merge -p '{"spec":{"managementState":"Removed"}}'
      
      # Wait for registry deployment to be deleted
      info "  Waiting for registry deployment to be removed..."
      for i in {1..24}; do
        if ! kubectl get deployment image-registry \
            -n openshift-image-registry >/dev/null 2>&1; then
          success "  Registry deployment removed"
          break
        fi
        if [ $i -eq 24 ]; then
          warning "  Registry deployment still exists, continuing anyway"
        fi
        sleep 5
      done
      
      # Step 2: Clear storage config to remove nested managementState
      info "  Step 2: Clearing storage configuration..."
      # Use JSON patch to completely remove the storage field
      retry_command kubectl patch \
        config.imageregistry.operator.openshift.io/cluster \
        --type json -p '[{"op":"remove","path":"/spec/storage"}]'
      
      sleep 2
      
      # Re-add empty storage to prepare for our PVC
      retry_command kubectl patch \
        config.imageregistry.operator.openshift.io/cluster \
        --type merge -p '{"spec":{"storage":{}}}'
      
      # Poll for operator to reconcile
      for i in {1..15}; do
        STORAGE_CHECK=$(kubectl get \
          config.imageregistry.operator.openshift.io/cluster \
          -o json | jq -r '.spec.storage.managementState // "none"')
        if [ "$STORAGE_CHECK" = "none" ]; then
          success "  Storage config cleared (no nested managementState)"
          break
        fi
        info "  Storage still has nested mgmt ($STORAGE_CHECK), waiting (attempt $i/15)..."
        sleep 2
      done
      
      # Step 3: Delete existing PVC (now that deployment is gone)
      if kubectl get pvc image-registry-storage \
          -n openshift-image-registry >/dev/null 2>&1; then
        info "  Step 3: Deleting existing PVC..."
        kubectl delete pvc image-registry-storage \
          -n openshift-image-registry
        
        for i in {1..30}; do
          if ! kubectl get pvc image-registry-storage \
              -n openshift-image-registry >/dev/null 2>&1; then
            success "  PVC deleted"
            break
          fi
          info "  Waiting for PVC deletion (attempt $i/30)..."
          sleep 2
        done
        
        if kubectl get pvc image-registry-storage \
            -n openshift-image-registry >/dev/null 2>&1; then
          warning "  PVC still exists after 60s, forcing deletion..."
          kubectl patch pvc image-registry-storage \
            -n openshift-image-registry \
            --type json -p '[{"op":"remove","path":"/metadata/finalizers"}]' 2>/dev/null || true
          sleep 5
        fi
      fi
      
      # Step 4: Create fresh PVC
      info "  Step 4: Creating PVC with $ACCESS_MODE mode..."
      cat <<EOFPVC | retry_command kubectl apply -f -
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: image-registry-storage
      namespace: openshift-image-registry
    spec:
      accessModes:
      - $ACCESS_MODE
      resources:
        requests:
          storage: 100Gi
      storageClassName: $STORAGE_CLASS
    EOFPVC
      
      # Wait for PVC to bind
      for i in {1..24}; do
        PVC_PHASE=$(kubectl get pvc image-registry-storage \
          -n openshift-image-registry \
          -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
        
        if [ "$PVC_PHASE" = "Bound" ]; then
          success "  PVC bound successfully"
          break
        fi
        info "  PVC status: $PVC_PHASE (attempt $i/24)..."
        sleep 5
      done
      
      if [ "$PVC_PHASE" != "Bound" ]; then
        error "PVC did not bind within 120s"
        kubectl describe pvc image-registry-storage \
          -n openshift-image-registry
        exit 1
      fi
      
      # Step 5: Configure registry to use PVC with Managed state
      info "  Step 5: Configuring registry storage with Managed state..."
      PATCH="{\"spec\":{\"storage\":{\"managementState\":\"Managed\",\"pvc\":{\"claim\":\"image-registry-storage\"}},\"replicas\":$REPLICAS,\"rolloutStrategy\":\"$ROLLOUT_STRATEGY\"}}"
      retry_command kubectl patch \
        config.imageregistry.operator.openshift.io/cluster \
        --type merge -p "$PATCH"
      
      # Verify configuration applied with Managed state
      for i in {1..15}; do
        STORAGE_SPEC=$(kubectl get \
          config.imageregistry.operator.openshift.io/cluster \
          -o json 2>/dev/null | jq -r '.spec.storage')
        STORAGE_MGMT=$(echo "$STORAGE_SPEC" | \
          jq -r '.managementState // "none"')
        PVC_CLAIM=$(echo "$STORAGE_SPEC" | \
          jq -r '.pvc.claim // "none"')
        
        if [ "$STORAGE_MGMT" = "Managed" ] && \
           [ "$PVC_CLAIM" = "image-registry-storage" ]; then
          success "  Storage configured (mgmt=Managed, pvc=configured)"
          break
        fi
        info "  Waiting (mgmt=$STORAGE_MGMT, pvc=$PVC_CLAIM, $i/15)..."
        sleep 2
      done
      
      success "Phase 3-4 complete: Storage configuration fixed"
    else
      info "Storage is already correctly configured"
    fi
    
    # ========================================================================
    # PHASE 5-6: CONFIGURE REGISTRY STATE
    # ========================================================================
    # Ensure registry is in Managed state and deployment is ready.
    # Restart controller-manager to clear any cached registry state.
    # ========================================================================
    
    info "Phase 5-6: Configuring registry state..."
    
    # Set to Managed state if needed
    CURRENT_STATE=$(kubectl get \
      config.imageregistry.operator.openshift.io/cluster \
      -o jsonpath='{.spec.managementState}' 2>/dev/null)
    
    if [ "$CURRENT_STATE" != "Managed" ]; then
      info "  Setting registry to Managed state..."
      retry_command kubectl patch \
        config.imageregistry.operator.openshift.io/cluster \
        --type merge -p '{"spec":{"managementState":"Managed"}}'
      
      for i in {1..15}; do
        STATE=$(kubectl get \
          config.imageregistry.operator.openshift.io/cluster \
          -o jsonpath='{.spec.managementState}' 2>/dev/null)
        if [ "$STATE" = "Managed" ]; then
          success "  Registry set to Managed"
          break
        fi
        info "  Waiting for Managed state (attempt $i/15)..."
        sleep 2
      done
    fi
    
    # Wait for deployment to be ready
    info "  Waiting for registry deployment to be ready..."
    for i in {1..24}; do
      READY=$(kubectl get deployment image-registry \
        -n openshift-image-registry \
        -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
      DESIRED=$(kubectl get deployment image-registry \
        -n openshift-image-registry \
        -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "0")
      
      if [ "$READY" = "$DESIRED" ] && [ "$READY" != "0" ]; then
        success "  Registry deployment ready: $READY/$DESIRED replicas"
        break
      fi
      
      if [ "$DESIRED" = "0" ]; then
        info "  Deployment not yet created (attempt $i/24)..."
      else
        info "  Registry replicas: $READY/$DESIRED (attempt $i/24)..."
      fi
      sleep 5
    done
    
    if [ "$READY" != "$DESIRED" ] || [ "$READY" = "0" ]; then
      error "Registry deployment not ready within 120s"
      kubectl get deployment image-registry \
        -n openshift-image-registry -o yaml
      kubectl get events -n openshift-image-registry \
        --sort-by=.metadata.creationTimestamp | tail -20
      exit 1
    fi
    
    # Wait for registry to stabilize before proceeding
    # Note: DO NOT restart controller-manager pods manually as this triggers
    # an operator reconciliation that performs a rolling deployment with a
    # new ReplicaSet. This can disrupt builds that are starting. The build
    # controller will discover the registry automatically.
    info "  Waiting for registry to stabilize..."
    sleep 15
    
    # Verify controller-manager pods are stable (not in the middle of a rollout)
    if kubectl get namespace openshift-controller-manager \
        >/dev/null 2>&1; then
      info "  Checking controller-manager stability..."
      
      # Check if there's an ongoing rollout
      ROLLOUT_STATUS=$(kubectl rollout status deployment/controller-manager \
        -n openshift-controller-manager --timeout=60s 2>&1 || echo "failed")
      
      if echo "$ROLLOUT_STATUS" | grep -q "successfully rolled out"; then
        success "  Controller-manager is stable and ready"
      else
        warning "  Controller-manager may be rolling out: $ROLLOUT_STATUS"
        warning "  Waiting for rollout to complete..."
        # Wait up to 2 minutes for any rollout to finish
        kubectl rollout status deployment/controller-manager \
          -n openshift-controller-manager --timeout=120s 2>/dev/null || true
      fi
    else
      info "  Controller-manager namespace not found (may be OKD/CRC)"
    fi
    
    success "Phase 5-6 complete: Registry is Managed and ready"
    
    # ========================================================================
    # PHASE 7: VERIFY BUILD FUNCTIONALITY
    # ========================================================================
    # Create a test ImageStream to verify build controller can interact
    # with the registry. This confirms the entire chain is working.
    # Clean up test resources when done.
    # ========================================================================
    
    info "Phase 7: Verifying build functionality..."
    
    # Wait for registry operator to complete internal reconciliation
    # The operator needs time to propagate the service hostname configuration
    # to ImageStream objects. We poll by creating a test ImageStream and
    # checking if it gets the dockerImageRepository field populated.
    #
    # Note: On OpenShift 4.18.30, .status.internalRegistryHostname may never
    # be populated, but ImageStreams will still work. We test actual ImageStream
    # functionality rather than relying on that field.
    info "  Waiting for registry operator to complete reconciliation..."
    info "  This may take several minutes on fresh clusters..."
    
    RECONCILE_TEST_IS="registry-reconcile-test-$RANDOM"
    oc create imagestream "$RECONCILE_TEST_IS" \
      -n jam-in-a-box >/dev/null 2>&1
    
    RECONCILE_SUCCESS=false
    for i in {1..300}; do
      DOCKER_REPO=$(oc get imagestream "$RECONCILE_TEST_IS" \
        -n jam-in-a-box \
        -o jsonpath='{.status.dockerImageRepository}' 2>/dev/null)
      
      if [ -n "$DOCKER_REPO" ]; then
        success "  Registry operator reconciliation complete (${i}s)"
        RECONCILE_SUCCESS=true
        break
      fi
      
      if [ $i -eq 1 ] || [ $((i % 10)) -eq 0 ]; then
        info "  Still waiting for ImageStream integration (${i}s elapsed)..."
      fi
      sleep 1
    done
    
    # Clean up reconciliation test ImageStream
    oc delete imagestream "$RECONCILE_TEST_IS" \
      -n jam-in-a-box >/dev/null 2>&1
    
    if [ "$RECONCILE_SUCCESS" != "true" ]; then
      error "Registry operator did not complete reconciliation within 5 minutes"
      error "ImageStreams may not work correctly for builds"
      error ""
      error "Debug commands:"
      error "  oc get config.imageregistry.operator.openshift.io/cluster -o yaml"
      error "  oc get deployment image-registry -n openshift-image-registry"
      error "  oc logs -n openshift-image-registry deployment/cluster-image-registry-operator --tail=50"
      exit 1
    fi
    
    # Now create the actual test ImageStream for Phase 7 verification
    TEST_IS="registry-test-$RANDOM"
    
    info "  Creating test ImageStream: $TEST_IS..."
    oc create imagestream "$TEST_IS" \
      -n jam-in-a-box >/dev/null 2>&1
    
    # Should get dockerImageRepository immediately now
    TEST_SUCCESS=false
    for i in {1..5}; do
      DOCKER_REPO=$(oc get imagestream "$TEST_IS" \
        -n jam-in-a-box \
        -o jsonpath='{.status.dockerImageRepository}' 2>/dev/null)
      
      if [ -n "$DOCKER_REPO" ]; then
        success "  Test ImageStream configured successfully"
        success "  Docker repository: $DOCKER_REPO"
        TEST_SUCCESS=true
        break
      fi
      sleep 1
    done
    
    # Clean up test ImageStream
    oc delete imagestream "$TEST_IS" \
      -n jam-in-a-box >/dev/null 2>&1 || true
    
    if [ "$TEST_SUCCESS" = "false" ]; then
      error "Test ImageStream did not configure within 30s"
      error "Build controller may not be able to interact with registry"
      oc get imagestream "$TEST_IS" -n jam-in-a-box -o yaml
      exit 1
    fi
    
    success "Phase 7 complete: Build controller can interact with registry"
    
    # CRITICAL: Wait for build controller cache to update
    # The build controller maintains an internal cache of registry configuration
    # that is separate from ImageStream functionality. Even though ImageStreams
    # work immediately, the build controller needs additional time to update its
    # cache. Without this wait, builds will fail with "registry is not configured".
    info "Waiting for build controller cache to update..."
    info "The build controller needs time to refresh its registry configuration."
    info "This wait prevents 'registry is not configured' errors during builds."
    sleep 120
    success "Build controller cache update period complete"
    
    success "=== Registry Configuration Complete ==="

  repo-clone.sh: |
    #!/bin/bash
    # Clone the jam-in-a-box repository
    set -e
    
    source /scripts/logs.sh
    source /tmp/env_setup
    
    # Check if archive-helper pod exists in jam-in-a-box namespace
    if kubectl get service archive-helper -n jam-in-a-box >/dev/null 2>&1; then

      warning "Found archive-helper service, using archive extraction method"
    
      ARCHIVE_URL=http://archive-helper.jam-in-a-box.svc.cluster.local/jam-in-a-box.tar
      info "Downloading repository archive from: $ARCHIVE_URL"
      
      # Download and extract the tarball
      curl -L "$ARCHIVE_URL" | tar -xf - --strip-components=1
      
      success "Repository archive downloaded and extracted"
      info "Repository contents:"
      ls -la
      
    else

      # Determine which branch to clone based on parameters
      BRANCH="main"
      if [[ "$PARAMETERS" == *"--canary="* ]]; then
        BRANCH=$(echo "$PARAMETERS" | grep -oP '(?<=--canary=)[^ ]+')
        info "Using branch: $BRANCH (from --canary parameter)"
      elif [[ "$PARAMETERS" == *"--canary"* ]]; then
        BRANCH="canary"
        info "Using canary branch based on --canary parameter"
      else
        info "Using main branch (default)"
      fi
      
      # Get the repository URL from repo-config.json
      repoUrl=""
      curl -L https://raw.githubusercontent.com/IBMIntegration/jam-in-a-box-2/refs/heads/main/repo-config.json \
        -o repo-config.json 2>/dev/null
      repoUrl="$(cat repo-config.json | jq -r '.template_vars.REPO_GIT_URL')"
      if [[ "$PARAMETERS" == *"--fork="* ]]; then
        FORK_NAME=$(echo "$PARAMETERS" | grep -oP '(?<=--fork=)[^ ]+')
        FORK_REPO_URL=$(cat repo-config.json | \
          jq -r --arg FORK_NAME "$FORK_NAME" \
          '.forks[$FORK_NAME].template_vars.REPO_GIT_URL // empty')
        if [ -n "$FORK_REPO_URL" ]; then
          repoUrl="$FORK_REPO_URL"
          info "Using fork repository URL: $repoUrl"
        else
          info "Fork '$FORK_NAME' not found in repo-config.json, using default repository"
        fi
      fi

      # Clone the repository with the determined branch
      info "Cloning repository from branch: $BRANCH"
      rm -rf .git ./* .[^.]* 2>/dev/null || true
      git clone -b $BRANCH $repoUrl .
      
      info "Repository cloned successfully. Contents:"
      ls -la
    fi
    
    # Make scripts executable
    info "Setting up permissions..."
    find . -name "*.sh" -exec chmod +x {} \;
    chmod +x main.sh
    
  setup.sh: |
    #!/bin/bash
    # Main setup orchestration script
    set -e
    
    # Source logging functions
    source /scripts/logs.sh
    
    # Install prerequisite tools
    /scripts/prereq-tools.sh
    
    # Install kubectl and oc
    /scripts/oc-tools.sh
    
    # Configure authentication
    /scripts/auth-config.sh

    # Validate cluster provisioning pipeline completed successfully
    /scripts/validate-pipeline-run.sh
    
    # Check for optional parameters ConfigMap first
    info "Checking for custom parameters..."
    PARAMETERS=""
    if kubectl get configmap jam-setup-params -n default; then
      info "Found custom parameters ConfigMap"
      PARAMETERS=$(kubectl get configmap jam-setup-params -n default \
        -o jsonpath='{.data.parameters}' 2>/dev/null || echo "")
    else
      info "No custom parameters found, using defaults"
    fi
    
    info "Parameters to use: ${PARAMETERS:-'(none)'}"
    echo "export PARAMETERS='${PARAMETERS}'" >> /tmp/env_setup

    # Set DEBUG flag based on parameters
    if [[ "$PARAMETERS" == *"--debug"* ]]; then
      DEBUG=true
    else
      DEBUG=${DEBUG:-false}
    fi
    echo "export DEBUG=$DEBUG" >> /tmp/env_setup

    echo "=== Integration Jam-in-a-Box Setup - Main Phase ==="
    
    # Check registry readiness
    /scripts/registry-check.sh
    
    # Verify registry is actually ready for builds
    info "Verifying registry is ready for builds..."
    
    # Check that registry pods are actually running and ready
    info "Checking registry pods..."
    # Use deployment readiness as the source of truth
    REGISTRY_READY_COUNT=$(kubectl get deployment image-registry -n openshift-image-registry \
      -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
    
    if [ "$REGISTRY_READY_COUNT" -eq 0 ]; then
      error "No ready registry pods found"
      kubectl get pods -n openshift-image-registry 2>/dev/null || true
      kubectl get deployment image-registry -n openshift-image-registry 2>/dev/null || true
      exit 1
    fi
    success "Registry has $REGISTRY_READY_COUNT ready pod(s)"
    
    # Check that openshift-controller-manager is ready (optional check)
    info "Checking openshift-controller-manager..."
    CONTROLLER_READY_COUNT=$(kubectl get pods -n openshift-controller-manager \
      --field-selector=status.phase=Running \
      --no-headers 2>/dev/null | wc -l || echo "0")
    
    if [ "$CONTROLLER_READY_COUNT" -eq 0 ]; then
      warning "No running controller-manager pods found, checking if namespace exists..."
      if kubectl get namespace openshift-controller-manager >/dev/null 2>&1; then
        warning "Namespace exists but no running pods - this may cause build issues"
        kubectl get pods -n openshift-controller-manager 2>/dev/null || true
      else
        info "openshift-controller-manager namespace doesn't exist (may be OKD/CRC)"
        info "Builds may still work if using a different controller"
      fi
    else
      success "Build controller has $CONTROLLER_READY_COUNT running pod(s)"
    fi
    
    # Give the build controller a moment to fully connect to registry
    info "Allowing build controller to stabilize connection to registry..."
    sleep 10
    
    success "Registry is fully ready for builds"
    
    # Clone repository
    cd /workspace
    /scripts/repo-clone.sh
    
    # Run the main setup script
    info "Starting main setup process..."
    info "Current directory: $(pwd)"
    info "Directory contents:"
    ls -la
    
    if [ -f "./main.sh" ]; then
      # Source the environment setup to ensure KUBECONFIG is available
      source /tmp/env_setup
      
      info "Executing: ./main.sh ${PARAMETERS}"
      info "/tmp/env_setup: $(cat /tmp/env_setup)"
      
      # Debug: Show current authentication status before running main.sh
      info "Debug: Current authentication status:"
      info "KUBECONFIG=$KUBECONFIG"
      retry_command kubectl auth whoami 2>/dev/null || \
        info "Using ServiceAccount token authentication"
      retry_command kubectl get pods --namespace=jam-in-a-box \
        >/dev/null 2>&1 && success "OpenShift access confirmed" || \
        warning "OpenShift access failed"
      
      # Run main.sh with retry logic for resilience
      # Ensure PATH is exported for subshells
      export PATH="/usr/local/bin:$PATH"
      if ! retry_command bash -c "source /tmp/env_setup && ./main.sh ${PARAMETERS}"; then
        error "main.sh failed after multiple retry attempts"
        SETUP_EXIT_CODE=1
      else
        SETUP_EXIT_CODE=0
      fi
      
      if [ $SETUP_EXIT_CODE -eq 0 ]; then
        echo "=== Setup completed successfully! ==="
        
        # Wait for navigator pod to be ready
        info "Waiting for jam-in-a-box navigator pod to be ready..."
        info "This may take several minutes as the image is pulled and the pod starts..."
        for i in {1..120}; do
          NAV_READY=$(retry_command kubectl get pods -n jam-in-a-box \
            -l app=navigator \
            -o jsonpath='{.items[0].status.conditions[?(@.type=="Ready")].status}' \
            2>/dev/null || echo "")
          
          if [ "$NAV_READY" = "True" ]; then
            success "Navigator pod is ready!"
            break
          else
            if [ $((i % 12)) -eq 0 ]; then
              info "Still waiting for navigator pod (${i} minutes elapsed)..."
            fi
            sleep 5
          fi
        done
        
        if [ "$NAV_READY" != "True" ]; then
          error "Navigator pod not ready after 10 minutes"
          error "This indicates a problem with the deployment."
          error ""
          error "Checking pod status:"
          retry_command kubectl get pods -n jam-in-a-box -l app=navigator
          error ""
          error "Checking pod events:"
          retry_command kubectl get events -n jam-in-a-box --sort-by=.metadata.creationTimestamp | tail -20
          exit 1
        else
          echo "=== jam-in-a-box is fully deployed and ready! ==="
        fi
      else
        echo "=== Setup failed with exit code: $SETUP_EXIT_CODE ==="
      fi
    else
      error "main.sh not found in cloned repository"
      ls -la
      exit 1
    fi
    
    # Keep container running for debugging/testing
    if $DEBUG; then
      info "Setup phase complete. Container will remain running for inspection."
      info "You can exec into this pod to test changes or debug issues:"
      info "  oc exec -it jam-setup-pod -n jam-in-a-box -- /bin/bash"
    else
      info "Setup phase complete. Container will exit shortly."
    fi
    info ""
    info "To clean up when done:"
    info "  oc delete deployment jam-in-a-box -n jam-in-a-box"
    info "  oc delete pod jam-setup-pod -n jam-in-a-box"
    info "  oc delete clusterrolebinding jam-setup-cluster-admin"
    info "  oc delete serviceaccount jam-setup-sa -n jam-in-a-box"
    info "  oc delete secrets --field-selector type=Opaque -o name -n jam-in-a-box"
    info "  oc delete -n jam-in-a-box cm jam-setup-scripts \\"
    info "    navigator-md-handler-config navigator-nginx-config \\"
    info "    navigator-scripts-init setup-output"
    
    echo
    echo "=== Access the navigator at: ==="
    echo $'\x1b[4;1m'
    navigator_hostname="$(
      kubectl -n jam-in-a-box get cm setup-output \
        -o jsonpath='{.data.setup\.json}' |
      jq -r '.[]
        |select(.kind=="Route" and .metadata.labels.app=="navigator")
        |.spec.host')"
    navigator_credentials=$(
      kubectl -n jam-in-a-box get secrets setup-secrets \
        -o jsonpath='{.data.secret\.json}' | base64 -d |
      jq -r '.[]|select(.metadata.name=="navigator-credentials")|.data')
    nuser=$(echo "$navigator_credentials" | jq -r '.username')
    npass=$(echo "$navigator_credentials" | jq -r '.password')
    echo "  https://$nuser:$npass@$navigator_hostname"
    echo $'\x1b[0m'
    echo "If credentials are required, use the following:"
    echo "  Username: $nuser"
    echo "  Password: $npass"
    echo
    if $DEBUG; then
      info "DEBUG mode is enabled, keeping container running indefinitely."
      while true; do
        sleep 3600
      done
    else
      info "DEBUG mode is disabled, exiting container."
      exit $SETUP_EXIT_CODE
    fi
---
# Main setup pod
apiVersion: v1
kind: Pod
metadata:
  name: jam-setup-pod
  namespace: jam-in-a-box
  labels:
    app: jam-setup
spec:
  serviceAccount: jam-setup-sa
  restartPolicy: Never
  
  # Shared volume for git repository
  volumes:
  - name: repo-volume
    emptyDir: {}
  - name: setup-scripts
    configMap:
      name: jam-setup-scripts
      defaultMode: 0755
  
  # Main container to run setup
  containers:
  - name: jam-setup
    image: registry.redhat.io/ubi8/nodejs-20:latest
    workingDir: /workspace
    volumeMounts:
    - name: repo-volume
      mountPath: /workspace
    - name: setup-scripts
      mountPath: /scripts
    
    # Set up environment
    env:
    - name: HOME
      value: "/tmp"
    
    command: ["/bin/bash", "/scripts/setup.sh"]
    
    # Security context for privileged operations
    securityContext:
      privileged: true
      runAsUser: 0
    
    # Resource limits
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "2Gi"
        cpu: "1000m"
